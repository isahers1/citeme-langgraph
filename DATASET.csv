id,excerpt,target_paper_title,target_paper_url,source_paper_title,source_paper_url,year,split
10,A second consideration is how to treat the image itself: the raw image could be fed directly into the reinforcement learning algorithm through a series of convolutions [CITATION].,Handwritten Digit Recognition with a Back-Propagation Network,https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf,Learning to Drive in a Day,https://arxiv.org/pdf/1807.00412.pdf,2018,test
12,"Unlike the cited prior works that strive to recover disentangled representations, InfoGAN requires no supervision of any kind. To the best of our knowledge, the only other unsupervised method that learns disentangled representations is hossRBM [CITATION], a higher-order extension of the spike-and-slab restricted Boltzmann machine that can disentangle emotion from identity on the Toronto Face Dataset [25].",Disentangling Factors of Variation via Generative Entangling,https://arxiv.org/pdf/1210.5474.pdf,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,https://arxiv.org/pdf/1606.03657.pdf,2016,test
13,This technique of lower bounding mutual information is known as Variational Information Maximization [CITATION]. ,The IM Algorithm : A variational approach to Information Maximization,https://aivalley.com/Papers/MI_NIPS_final.pdf,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,https://arxiv.org/pdf/1606.03657.pdf,2016,train
16,"The rationale for this analysis was based on the in variance of the mutual information to invertible re-parameterization and on the Data Processing Inequalities along the Markov chain of the layers. Moreover, they suggested that optimized DNNs layers should approach the Information Bottleneck (IB) bound [CITATION] of the optimal achievable representations of the input X",Deep Learning and the Information Bottleneck Principle,https://arxiv.org/pdf/1503.02406.pdf,Opening the Black Box of Deep Neural Networks via Information,https://arxiv.org/pdf/1703.00810.pdf,2017,test
21,"One reason might be the training task itself: from [CITATION] we know that ImageNet can be solved to high accuracy using only local information. In other words, it might simply suffice to integrate evidence from many local texture features rather than going through the process of integrating and classifying global shapes",Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,https://arxiv.org/pdf/1904.00760.pdf,ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness,https://arxiv.org/pdf/1811.12231.pdf,2019,train
24,"To address this, [CITATION] introduced Adversarial Filtering (AF). An overview is shown in Figure 2. The key idea is to produce a dataset D which is adversarial for any arbitrary split of (D_train), (D_test)",Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,https://aclanthology.org/D18-1009.pdf,HellaSwag: Can a Machine Really Finish Your Sentence?,https://arxiv.org/pdf/1905.07830.pdf,2019,test
25,"To investigate this, we perform AF using BERT Large as the discriminator in two settings, comparing generations from Zellers et al. (2018) with those from a finetuned GPT [CITATION]",Improving Language Understanding by Generative Pre-Training,https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf,HellaSwag: Can a Machine Really Finish Your Sentence?,https://arxiv.org/pdf/1905.07830.pdf,2019,test
27,"Even if we assume fixed filters using a combination of the above, our probabilistic formulation still allows learning the parameters of the GSM experts from data as outlined below. Consequently, we do not need to tune the trade-off weights between the brightness and gradient constancy terms by hand as in [CITATION].",High Accuracy Optical Flow Estimation Based on a Theory for Warping,https://www.mia.uni-saarland.de/Publications/brox-eccv04-of.pdf,Learning Optical Flow,https://files.is.tue.mpg.de/black/papers/SunECCV08old.pdf,2008,train
28,"[CITATION], arguably the most common non-parametric calibration method, learns a piecewise constant function f to transform uncalibrated outputs; i.e. qi = f (pi). Specifically, isotonic regression produces f to minimize the square loss ∑(f (ˆpi) − yi)^2. Because f is constrained to be piecewise constant, we can write the optimization problem as",Transforming classifier scores into accurate multiclass probability estimates,https://dl.acm.org/doi/pdf/10.1145/775047.775151,On Calibration of Modern Neural Networks,https://arxiv.org/pdf/1706.04599.pdf,2017,test
31,"Using Gaussian noise and blur, [CITATION] demonstrate the superior robustness of human vision to convolutional networks, even after networks are fine-tuned on Gaussian noise or blur.",A study and comparison of human and deep learning recognition performance under visual distortions,https://arxiv.org/pdf/1705.02498.pdf,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,https://arxiv.org/pdf/1903.12261.pdf,2019,test
32,"Several studies demonstrate the fragility of convolutional networks on simple corruptions. For example, [CITATION] apply impulse noise to break Google’s Cloud Vision API.",Google’s cloud vision api is not robust to noise,https://arxiv.org/abs/1704.05051,Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,https://arxiv.org/pdf/1903.12261.pdf,2019,test
41,"We introduce Double Quantization (DQ), the process of quantizing the quantization constants for additional memory savings. While a small blocksize is required for precise 4-bit quantization [CITATION], it also has a considerable memory overhead.",The case for 4-bit precision: k-bit inference scaling laws,https://arxiv.org/pdf/2212.09720.pdf,QLORA: Efficient Finetuning of Quantized LLMs,https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf,2023,test
48,"Similar to our work is that of [CITATION], who observe high accuracy on task A on the linear path between a model which achieves high accuracy on task A and a model which is fine-tuned jointly on task A and B.",Linear Mode Connectivity in Multitask and Continual Learning,https://arxiv.org/pdf/2010.04495.pdf,Patching open-vocabulary models by interpolating weights,https://proceedings.neurips.cc/paper_files/paper/2022/file/bc6cddcd5d325e1c0f826066c1ad0215-Paper-Conference.pdf,2022,test
54,"A user may want to control the amount of snow on the mountain. However, it is quite difficult to describe the desired amount of snow through text. Instead, we suggest a fader control [CITATION], where the user controls the magnitude of the effect induced by a specific word, as depicted in fig. 9. ",Fader networks: Manipulating images by sliding attributes,https://arxiv.org/pdf/1706.00409.pdf,Prompt-to-Prompt Image Editing with Cross Attention Control,https://arxiv.org/pdf/2208.01626.pdf,2022,test
56,"The model was trained on Gaussian patches [CITATION]. Since no model weights are released, we can only include their Top-1 ImageNet-C accuracy values from their paper (and not the Top-5).",Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation,https://arxiv.org/pdf/1906.02611.pdf,A simple way to make neural networks robust against diverse image corruptions,https://arxiv.org/pdf/2001.06057.pdf,2020,test
59,"Our evaluation follows the protocol of [CITATION], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]",Resolution-robust Large Mask Inpainting with Fourier Convolutions,https://arxiv.org/pdf/2109.07161.pdf,High-Resolution Image Synthesis with Latent Diffusion Models,https://arxiv.org/pdf/2112.10752.pdf,2022,test
68,"The Internet has fueled collecting billions of (alt-text, image) pairs from HTML pages [CITATION], enabling the recent breakthroughs in Text-to-Image (T2I) modeling",LAION-5B: An open large-scale dataset for training next generation image-text models,https://arxiv.org/pdf/2210.08402.pdf,Make-A-Video: Text-to-Video Generation without Text-Video Data,https://arxiv.org/pdf/2209.14792.pdf?utm_source=webtekno,2022,test
69,"This attack [CITATION] uses a task-ignoring text (e.g., “Ignore my previous instructions.”) to explicitly tell the LLM that the target task should be ignored. Specifically, given the target data $x^t$ , injected instruction $s^e$, and injected data $x^e$, this attack crafts \tilde{x} by appending a task-ignoring text to $x^t$ before concatenating with ssse and $x^e$.",Ignore Previous Prompt: Attack Techniques For Language Models,https://arxiv.org/pdf/2211.09527.pdf,Prompt Injection attack against LLM-integrated Applications,https://arxiv.org/pdf/2306.05499.pdf,2024,test
72,"We learn π using behavioral cloning [CITATION], which optimizes π by minimizing the negative log-likelihood of actions at given the images and language instructions.",Alvinn: An autonomous land vehicle in a neural network,https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf,RT-1: Robotics Transformer for Real-World Control at Scale,https://arxiv.org/pdf/2212.06817.pdf,2023,train
77,"Multiple studies conclude that ID and OOD performance vary jointly across models on many real-world datasets [ 6, 24 , 41]. [CITATION] famously report an almost-systematic linear correlation between probit-scaled ID and OOD accuracies.",Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization,http://proceedings.mlr.press/v139/miller21b/miller21b.pdf,ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets,https://arxiv.org/pdf/2209.00613.pdf,2023,test
78,"On the WildTime-arXiv [53] and waterbirds datasets [CITATION], we observe wide variations in both ID and OOD performance across seeds, epochs, and methods",Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization,https://arxiv.org/pdf/1911.08731.pdf,ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets,https://arxiv.org/pdf/2209.00613.pdf,2023,test
85,"We explain this undesired hackability in existing benchmarks by showcasing that there exists a significant distributional gap between the positive and hard negative captions. For instance, in the ARO benchmark [CITATION], human-generated positive captions differ drastically from the hard negative texts generated by randomly shuffling words in the positive captions.","When and why vision-language models behave like bags-of-words, and what to do about it?",https://arxiv.org/pdf/2210.01936.pdf,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,https://arxiv.org/pdf/2306.14610.pdf,2023,test
86,"To mitigate this relational understanding issue, a composition-aware hard negative mining strategy (NegCLIP) is introduced in [CITATION]. Note that this strategy is extremely lightweight, and can be seamlessly integrated as an additional fine-tuning stage in enhancing CLIP’s text understanding ability","When and why vision-language models behave like bags-of-words, and what to do about it?",https://arxiv.org/pdf/2210.01936.pdf,An Inverse Scaling Law for CLIP Training,https://arxiv.org/pdf/2305.07017.pdf,2023,test
87,"Specifically, although [CITATION] first demonstrated that there exists a modality gap between text and image embeddings generated from VLMs, the geometry of this modality gap permits cross-modality transferability. This phenomenon allows text to serve as a proxy to corresponding images and vice versa.",Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,https://arxiv.org/pdf/2203.02053.pdf,LOVM: Language-Only Vision Model Selection,https://arxiv.org/pdf/2306.08893.pdf,2023,train
88,"More specifically, these strategies employ Plackett-Luce (PL) ranking models [38, 48] to enhance the likelihood of achieving the most accurate item ordering. In RankCLIP, we incorporate ListMLE [CITATION] as part of our training objective to optimize both in-modal and cross-modal ranking consistencies.",Learning to Rank: From Pairwise Approach to Listwise Approach,http://icml2008.cs.helsinki.fi/papers/167.pdf,RankCLIP: Ranking-Consistent Language-Image Pretraining,https://arxiv.org/pdf/2404.09387.pdf,2024,test
89,"Although such downstream task is unknown in advance, it is possible to estimate via some proxy dataset as prior. This was first proposed in [CITATION] relying on sampling image text pairs that are semantically similar to diverse and curated datasets like ImageNet (Deng et al., 2009).",DataComp: In search of the next generation of multimodal datasets,https://arxiv.org/pdf/2304.14108.pdf,Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning,https://arxiv.org/pdf/2402.02055.pdf,2024,test
90,"We introduce TIC-DataComp, a new benchmark for Time-Continual training of CLIP models, which we create by appending “crawl time” information to existing CommonPool dataset [CITATION].",DataComp: In search of the next generation of multimodal datasets,https://arxiv.org/pdf/2304.14108.pdf,TiC-CLIP: Continual Training of CLIP Models,https://arxiv.org/pdf/2310.16226.pdf,2023,test
91,"Our experiments are conducted on a large image-and-language dataset, WebLI [CITATION], which contains over 3.4 billion image-text pairs in English",PaLI: A Jointly-Scaled Multilingual Language-Image Model,https://arxiv.org/abs/2209.06794,"Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies",https://arxiv.org/abs/2404.08197,2024,test
92,Recent focus has been on synthetic methods like Deductive Databases (DD) [CITATION] that mimic human-like proving techniques and produce intelligible proofs by treating the problem of theorem proving as a step-by-step search problem using a set of geometry axioms. ,A Deductive Database Approach to Automated Geometry Theorem Proving and Discovering,https://link.springer.com/article/10.1023/A:1006171315513,Wu's Method can Boost Symbolic AI to Rival Silver Medalists and AlphaGeometry to Outperform Gold Medalists at IMO Geometry,https://arxiv.org/abs/2404.06405,2024,test
93,[CITATIONS] introduce emergence as a “behavior of a system [that] is implicitly induced rather than explicitly constructed; it is both the source of scientific excitement and anxiety about unintended consequences.”,On the Opportunities and Risks of Foundation Models,https://arxiv.org/abs/2108.07258,Harms from Increasingly Agentic Algorithmic Systems,https://arxiv.org/abs/2302.10329,2023,test
96,"LOVM (Zohar et al., 2023) and Flash-HELM [CITATION] similarly rank foundation models efficiently on unseen datasets.",Efficient Benchmarking (of Language Models) ,https://arxiv.org/abs/2308.11696v5,Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress,https://arxiv.org/abs/2402.19472,2024,test
100,[CITATION] revealed exact match contamination rates ranging from under 2% to over 50% on various GLUE benchmarks when compared to the C4 pretraining data,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,https://arxiv.org/pdf/2104.08758.pdf,An Open Source Data Contamination Report for Large Language Models,https://arxiv.org/pdf/2310.17589.pdf,2023,test
102,"Subsequent versions like CIFAR10.1 [CITATION], CIFAR10.2 (Lu et al., 2020), CINIC10 (Darlow et al., 2018), and CIFAR10-W (Sun et al.,2023) introduced more challenging and diverse samples to evaluate the same objective of classifying 10 categories.",Do CIFAR-10 Classifiers Generalize to CIFAR-10?,https://arxiv.org/pdf/1806.00451.pdf,Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress,https://arxiv.org/pdf/2402.19472.pdf,2024,test
104,Sentiment analysis: we adopt the SST-2 [CITATION] dataset from the GLUE ,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf,PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts,https://arxiv.org/abs/2306.04528,2023,test
107,Existing tools and datasets can be divided by proof assistants: Lean has LeanStep [CITATION] and lean-gym [19 ],Proof Artifact Co-training for Theorem Proving with Language Models,https://arxiv.org/abs/2102.06203,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,https://arxiv.org/abs/2306.15626,2023,test
108,"On MiniF2F’s test set in Lean, ReProver achieves a Pass@1 of 26.5%, which is competitive with state-of-the-art methods without RL (25.9% in [CITATION]).",Formal Mathematics Statement Curriculum Learning,https://arxiv.org/abs/2202.01344v1,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,https://arxiv.org/abs/2306.15626,2023,test
109,Zephyr-7B-Beta [CITATION] is an instruction-tuned version of Mistral-7B,Zephyr: Direct Distillation of LM Alignment,https://arxiv.org/abs/2310.16944,Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection,https://arxiv.org/abs/2308.10819,2023,test
110,"Note that, [CITATION] further scales up the size of RETRO to 48B and discusses how instruction tuning can help improve retrieval-augmented LLMs for zero-shot open-domain question answering.",InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining,https://arxiv.org/abs/2310.07713,Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study,https://arxiv.org/abs/2304.06762,2023,test
111,"Recall that the loss for a k-means problem instance is given by the sum of squared Euclidean distance from each datapoint to its nearest centroid. Let $L^∗$ be the optimal loss for a particular problem instance. Achieving the optimal solution is, in general, NP-Hard [CITATION]. ",NP-hardness of Euclidean sum-of-squares clustering,https://link.springer.com/article/10.1007/s10994-009-5103-0,Making AI Forget You: Data Deletion in Machine Learning,https://arxiv.org/abs/1907.05012,2019,test
113,We compare the bound in Theorem 1.1 to a baseline Lipschitz bandits regret bound. The concept of zooming dimension stems from the work of [CITATION] and serves as an instance-dependent notion of dimensionality.,Multi-Armed Bandits in Metric Spaces,https://arxiv.org/abs/0809.4882,Regret Minimization with Performative Feedback,https://proceedings.mlr.press/v162/jagadeesan22a/jagadeesan22a.pdf,2022,test
116,"The discourse vector $c_t$ does a slow random walk (meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector), so that nearby words are generated under similar discourses. It was shown in [CITATION] that under some reasonable assumptions this model generates behavior –in terms of word-word cooccurrence probabilities—that fits empirical works like word2vec and Glove.",A Latent Variable Model Approach to PMI-based Word Embeddings,https://arxiv.org/abs/1502.03520,A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS,https://oar.princeton.edu/bitstream/88435/pr1rk2k/1/BaselineSentenceEmbedding.pdf,2017,test
117,"For example, the sample complexity of DP-PCA(Chaudhuri et al., 2012) is dependent on the dimension of the training data [CITATION], implying that the costs associated with DP-PCA could surpass the benefits of private learning in a lower-dimensional space.",DP-PCA: Statistically Optimal and Differentially Private PCA,https://arxiv.org/abs/2205.13709,Provable Privacy with Non-Private Pre-Processing,https://arxiv.org/abs/2403.13041,2024,test
120,"[CITATION] advocates the development of a theory of clustering that will be “independent of any particular algorithm, objective function, or generative data model. They suggest three axioms, aimed to define what a clustering function is, each sounding plausible, and shows that these seemingly natural axioms lead to a contradiction - there exists no function that satisfies all three requirements.",An Impossibility Theorem for Clustering,https://dl.acm.org/doi/10.5555/2968618.2968676,Measures of Clustering Quality: A Working Set of Axioms for Clustering,https://proceedings.neurips.cc/paper_files/paper/2008/file/beed13602b9b0e6ecb5b568ff5058f07-Paper.pdf,2008,test
121,"[CITATION] show: No computable learner can non-uniformly learn the class $\mathcal{H}_{comp}$, the class of all computable functions from $\mathbb{N}$ to {0, 1}",Statistical learning of arbitrary computable classifiers.,https://arxiv.org/abs/0806.3537,On Learnability with Computable Learners,https://proceedings.mlr.press/v117/agarwal20b/agarwal20b.pdf,2008,train
122,"Significant progress was made by [CITATION], who gave a characterization of proper strong computable PAC learning in terms of the computability of a Empirical Risk Minimizer (ERM) and who constructed a class of finite VC dimension which is not computable PAC learnable, even in the improper sense. ",On characterizations of learnability with computable learners,https://arxiv.org/abs/2202.05041,Find a witness or shatter: the landscape of computable PAC learning,https://arxiv.org/abs/2302.04731,2023,train
123,ToolLLaMA exhibits strong generalization performance on an out-of-distribution (OOD) dataset APIBench [CITATION],Gorilla: Large Language Model Connected with Massive APIs,https://arxiv.org/abs/2305.15334,ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs,https://arxiv.org/abs/2307.16789,2023,test
126,DCLIP [CITATION] requires external LLMs for descriptors that convert the single-class matching problem to one over an ensemble of fine-grained class representations.,Visual Classification via Description from Large Language Models,https://arxiv.org/pdf/2210.07183.pdf,Waffling around for Performance: Visual Classification with Random Words and Broad Concepts,https://arxiv.org/pdf/2306.07282.pdf,2023,test
128,"Contrastive vision-language representation learning has recently emerged as an effective technique to learn representations with weak supervision that work for a wide range of tasks and have intriguing properties, such as strong zero-shot abilities. However, our understanding of the learned representation is in its infancy. For instance, recent work showed the presence of a modality gap and attributed it to the cone effect of model initialization and the contrastive loss [CITATION].",Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,https://arxiv.org/pdf/2203.02053,"Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning",https://arxiv.org/pdf/2404.07983,2022,test
129,"By default, we use 900 queries in our model following DINO. We set the maximum text token number as 256. Using BERT as our text encoder, we follow BERT to tokenize texts with a BPE scheme [CITATION]. We use six feature enhancer layers in the feature enhancer module. The cross-modality decoder is composed of six decoder layers as well.",Neural Machine Translation of Rare Words with Subword Units,https://arxiv.org/pdf/1508.07909.pdf,Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection,https://arxiv.org/pdf/2303.05499.pdf,2023,test
131,"We use popular downstream tasks such as ILSVRC2012 “ImageNet” (1.3M training examples, 1k classes) with the original validation labels [13] and cleaned-up ReaL labels [CITATION], CIFAR-10/100 (50k examples, 10/100 classes) [23], Oxford-IIIT Pets (3.7k examples, 36 classes) [33], and Oxford Flowers-102 (2k examples, 102 classes) [32",Are we done with ImageNet?,https://arxiv.org/pdf/2006.07159.pdf,MLP-Mixer: An all-MLP Architecture for Vision,https://arxiv.org/pdf/2105.01601.pdf,2021,test
135,"This can be observed in the popularity of the field-wide concept of scaling laws, where large scale is often juxtaposed with better model performance [35]. Following analysis of the top 100 most influential ML papers from the past decade published in two of the most prestigious ML conferences (NeurIPS and ICML) [CITATION], for example, found that “scaling up” is one of the top sought out values in ML research.",The Values Encoded in Machine Learning Research,https://arxiv.org/pdf/2106.15590.pdf,Into the LAIONs Den: Investigating Hate in Multimodal Datasets,https://arxiv.org/pdf/2311.03449.pdf,2023,test
136,Faiss was open-sourced simultaneously with the release of the paper [CITATION] that describes the GPU implementation of several index types. The present paper complements this previous work by describing the library as a whole.,Billion-scale similarity search with GPUs.,https://arxiv.org/abs/1702.08734,The Faiss library,https://arxiv.org/pdf/2401.08281.pdf,2024,test
137,"Other work [CITATION] has shown the presence of explicit images, pornography, malign stereotypes, and other extremely problematic content in the LAION-400M dataset.","Multimodal datasets: misogyny, pornography, and malignant stereotypes",https://arxiv.org/pdf/2110.01963.pdf,Into the LAIONs Den: Investigating Hate in Multimodal Datasets,https://arxiv.org/pdf/2311.03449.pdf,2023,test
139,We extract the frequencies of a set of 22 regular expressions related to identity mentions and compute the pointwise mutual information (PMI; [CITATION]) between the likelihood of an identity mention occurring versus being filtered out by the blocklist.,"Word Association Norms, Mutual Information, and Lexicography",https://aclanthology.org/J90-1003.pdf,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,https://arxiv.org/pdf/2104.08758.pdf,2021,test
140,"We use the large-scale online continual learning dataset CLOC [CITATION], which contains 39 million time-stamped images exhibiting natural distribution shifts. The task is to identify the geolocation of a given image where the total number of geolocation labels is 712.",Online Continual Learning with Natural Distribution Shifts: An Empirical Study with Visual Data,https://arxiv.org/abs/2108.09020,Real-Time Evaluation in Online Continual Learning: A New Hope,https://openaccess.thecvf.com/content/CVPR2023/papers/Ghunaim_Real-Time_Evaluation_in_Online_Continual_Learning_A_New_Hope_CVPR_2023_paper.pdf,2023,test
146,In the spirit of transductive bandits [CITATION] we consider a more general setting where answers are sets of arms. The set of actions and the set of answers can be different,Sequential Experimental Design for Transductive Linear Bandits,https://arxiv.org/abs/1906.08399,Efficient Pure Exploration for Combinatorial Bandits with Semi-Bandit Feedback,https://arxiv.org/abs/2101.08534,2021,test
147,"We prove two lower bounds for the complexity of non-log-concave sampling within the framework of [CITATION], who introduced the use of Fisher information (FI) bounds as a notion of approximate first-order stationarity in sampling.",Towards a Theory of Non-Log-Concave Sampling: First-Order Stationarity Guarantees for Langevin Monte Carlo,https://arxiv.org/abs/2202.05214,Fisher information lower bounds for sampling,https://arxiv.org/abs/2210.02482v1,2022,test
152,"To the best of our knowledge, there is only one method, QII, that provably provides differentially private black-box post-hoc model explanations [CITATION], protecting the explanation data. QII introduces Shapley value based model explanations, which have become a popular model explanation framework",Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems,https://ieeexplore.ieee.org/document/7546525,Model Explanations with Differential Privacy,https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533235,2022,train
153,"With the success of various attacking methods, robustness becomes an important consideration in watermarking techniques. However, [CITATION] proves that it is only feasible to achieve robustness to a well-specified set of attacks, instead of all. This fact aligns with our Theorem 4.4, which characterizes the fundamental limits of robust watermarking under different attacking powers.",Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models,https://arxiv.org/abs/2311.04378,Towards Optimal Statistical Watermarking,https://arxiv.org/abs/2312.07930,2023,train
154,Hummingbird [CITATION] suggests employing a straightforward non-parametric nearest neighbor retrieval as the decoder for dense scene understanding tasks,Towards In-context Scene Understanding,https://arxiv.org/abs/2306.01667,kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies,https://arxiv.org/abs/2404.09447,2024,test
161,"For a comprehensive evaluation, we test on 19 datasets spanning a wide range of object, scene and fine-grained categories: ImageNet [18], StanfordCars [43], UCF101 [68], Caltech101 [25], Caltech256 [32], Flowers102 [56], OxfordPets [CITATION], Food101 [7], SUN397 [75], DTD [14], EuroSAT [37], FGVCAircraft [51], Country211 [61], CIFAR-10 [44], CIFAR-100 [44], Birdsnap [5], CUB [72], ImageNet-Sketch [73] and ImageNet-R [38].",Cats and Dogs,https://ieeexplore.ieee.org/abstract/document/6248092,SuS-X: Training-Free Name-Only Transfer of Vision-Language Models,https://arxiv.org/pdf/2211.16198.pdf,2022,test
164,"KD-DTI is dataset for drug-target-interaction introduced by [CITATION], consisting of 12k/1k/1.3k documents as the train/validation/test set. ",Discovering drug-target interaction knowledge from biomedical literature,https://arxiv.org/pdf/2109.13187,BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,https://arxiv.org/pdf/2210.10341,2023,test
168,[CITATION] empirically study calibration under distribution shift and provide a large comparison of methods for improving calibration. They report that both accuracy and calibration deteriorate with distribution shift. ,Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift,https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf,Revisiting the Calibration of Modern Neural Networks,https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf,2021,train
169,"IMAGENET-R [CITATION] contains artificial renditions of IMAGENET classes such as art, cartoons, drawings, sculptures, and others",The many faces of robustness: A critical analysis of out-of-distribution generalization,https://arxiv.org/pdf/2006.16241,Revisiting the Calibration of Modern Neural Networks,https://proceedings.neurips.cc/paper/2021/file/8420d359404024567b5aefda1231af24-Paper.pdf,2021,test
173,"In the context of LLMs, we propose to constrain the strength of the attacker by limiting their computational budget in terms of the number of model evaluations. Existing attacks, such as GCG [CITATION], are already 5-6 orders of magnitude more expensive than attacks in computer vision.",Universal and Transferable Adversarial Attacks on Aligned Language Models,https://arxiv.org/pdf/2307.15043,Baseline Defenses for Adversarial Attacks Against Aligned Language Models,https://arxiv.org/pdf/2309.00614,2023,test
177,"We define the robotics data mixture used across all of the experiments as the data from 9 manipulators, and taken from RT-1 [8], QT-Opt [66], Bridge [95], Task Agnostic Robot Play [126, 127], Jaco Play [128], Cable Routing [CITATION], RoboTurk [86], NYU VINN [130], Austin VIOLA [131], Berkeley Autolab UR5 [132], TOTO [133] and Language Table [91] datasets.",Multi-Stage Cable Routing through Hierarchical Imitation Learning,https://arxiv.org/pdf/2307.08927,Open X-Embodiment: Robotic Learning Datasets and RT-X Models,https://arxiv.org/pdf/2310.08864v4,2023,test
179,"We conclude by discussing initial use-cases of mmc4, including OpenFlamingo [CITATION], an open source version of Flamingo [2].",OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models,https://arxiv.org/pdf/2308.01390,"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text",https://proceedings.neurips.cc/paper_files/paper/2023/file/1c6bed78d3813886d3d72595dbecb80b-Paper-Datasets_and_Benchmarks.pdf,2023,test
181,All backbones in this section are pre-trained on ImageNet-21k [CITATION],Imagenet: A large-scale hierarchical image database,https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf,Visual Prompt Tuning,https://arxiv.org/pdf/2203.12119,2022,test
182,"Two predictions immediately follow from this hypothesis: (1.) error consistency between two identical models trained on very different datasets, such as ImageNet vs. Stylized-ImageNet, is much lower than error consistency between very different models (ResNet-50 vs. VGG-16) trained on the same dataset. (2.) error consistency between ResNet-50 and a highly flexible model (e.g., a vision transformer) is much higher than error consistency between ResNet-50 and a highly constrained model like BagNet-9 [CITATION]",Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet,https://arxiv.org/pdf/1904.00760,Partial success in closing the gap between human and machine vision,https://proceedings.neurips.cc/paper/2021/file/c8877cff22082a16395a57e97232bb6f-Paper.pdf,2021,test
183,"To this end, error consistency (a.k.a. Cohen’s kappa, cf. [CITATION]) indicates whether the observed consistency is larger than what could have been expected given two independent binomial decision makers with matched accuracy, which we denote as ˆoh,m.",A coefficient of agreement for nominal scales,https://journals.sagepub.com/doi/10.1177/001316446002000104,Partial success in closing the gap between human and machine vision,https://proceedings.neurips.cc/paper/2021/file/c8877cff22082a16395a57e97232bb6f-Paper.pdf,2021,test
184,"For CUB, we also use sentence embeddings extracted from 10 sentences annotated per image averaged per class [23] and for ImageNet we used Word2Vec [CITATION] embeddings provided by [4]",Distributed representations of words and phrases and their compositionality,https://arxiv.org/pdf/1310.4546,Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders,https://openaccess.thecvf.com/content_CVPR_2019/papers/Schonfeld_Generalized_Zero-_and_Few-Shot_Learning_via_Aligned_Variational_Autoencoders_CVPR_2019_paper.pdf,2019,test
185,Typical class-embeddings are vectors of hand-annotated continuous attributes or Word2Vec features [CITATIONS].,Distributed representations of words and phrases and their compositionality,https://arxiv.org/pdf/1310.4546,Generalized Zero- and Few-Shot Learning via Aligned Variational Autoencoders,https://openaccess.thecvf.com/content_CVPR_2019/papers/Schonfeld_Generalized_Zero-_and_Few-Shot_Learning_via_Aligned_Variational_Autoencoders_CVPR_2019_paper.pdf,2019,test
187,"For readability scores, we report the Simple Measure of Gobbledygook (SMOG) score [CITATION] and the Flesch Reading Ease (FRE) score [28]",SMOG Grading-a New Readability Formula,https://ogg.osu.edu/media/documents/health_lit/WRRSMOG_Readability_Formula_G._Harry_McLaughlin__1969_.pdf,DOCCI: Descriptions of Connected and Contrasting Images,https://arxiv.org/pdf/2404.19753,2024,test
191,"The pioneering work of Reed et al. [37] approached text-guided image generation by training a conditional GAN [CITATION], conditioned by text embeddings obtained from a pretrained encoder.",Conditional generative adversarial nets,https://arxiv.org/pdf/1411.1784,StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery,https://arxiv.org/pdf/2103.17249,2021,test
192,"For this task, we use the Comparative Question Completion dataset introduced by [CITATION]. This consists of questions in which one of a pair of coordinated elements is masked; the target is the masked phrase","What’s the best place for an ai conference, vancouver or ___: Why completing comparative questions is difficult ",https://arxiv.org/pdf/2104.01940,Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding,https://arxiv.org/pdf/2303.12513,2023,train
198,A number of independent and concurrent works have recently emerged which address the problem of audio-visual sound source separation using deep neural networks. [CITATION] first train a network to predict whether audio and visual streams are temporally aligned.,Audio-Visual Scene Analysis with Self-Supervised Multisensory Features,https://arxiv.org/pdf/1804.03641.pdf,Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation,https://arxiv.org/pdf/1804.03619.pdf,2018,test
199,[CITATION] define a tool as “an external module that is typically called using a rule or a special token and whose output is included in the augmented LM’s context.”,Augmented Language Models: a Survey,https://arxiv.org/abs/2302.07842,What Are Tools Anyway? A Survey from the Language Model Perspective,https://arxiv.org/pdf/2403.15452,2024,test
200,"Program synthesis can be posed as a coorporative communicative game: the user chooses an informative set of examples to convey the program to the synthesizer, and the synthesizer chooses a program under the assumption that these examples were chosen informatively. Models of pragmatic inference, specifically, the Rational Speech Acts (RSA) framework [CITATION] can then be used to build a program synthesizer that can resolve ambiguity via recursive Bayesian inference. ",Predicting pragmatic reasoning in language games.,https://web.stanford.edu/~ngoodman/papers/FrankGoodman-Science2012.pdf,Generating Pragmatic Examples to Train Neural Program Synthesizers,https://arxiv.org/abs/2311.05740,2023,test
202,"The DQN was a significant advance in reinforcement learning, showing that a single algorithm can learn to play a wide variety of complex tasks. The network was trained to play 49 classic Atari games, proposed as a test domain for reinforcement learning [CITATION], impressively achieving human-level performance or above on 29 of the games. ",The Arcade Learning Environment: An Evaluation Platform for General Agents,https://arxiv.org/abs/1207.4708,Building Machines That Learn and Think Like People,https://arxiv.org/pdf/1604.00289,2016,test
203,"[CITATION] trained a deep convolutional network-based system (PhysNet) to predict the stability of block towers from simulated images similar to those in Figure 4A but with much simpler configurations of two, three or four cubical blocks stacked vertically.",Learning Physical Intuition of Block Towers by Example.,arxiv.org/abs/1603.01312,Building Machines That Learn and Think Like People,https://arxiv.org/pdf/1604.00289,2016,test
204,"Also closely related to our hypothesis is the “Anna Karenina scenario” described by [CITATION], referring to the possibility that all well-performing neural nets represent the world in the same way. We discuss the evidence they give for this possibility in Section 2",Revisiting Model Stitching to Compare Neural Representations,https://arxiv.org/pdf/2106.07682,The Platonic Representation Hypothesis,https://arxiv.org/pdf/2405.07987,2024,test
205,"This harder setting is particularly interesting for editing methods such as MEMIT, which can handle large numbers of edits effectively [CITATION].",Mass-Editing Memory in a Transformer,https://arxiv.org/abs/2210.07229,MQUAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,https://aclanthology.org/2023.emnlp-main.971.pdf,2023,test
206,Kernel Calibration (Section 8) The notion of kernel calibration error was introduced in [CITATION] as Maximum Mean Calibration Error (MMCE). ,Trainable Calibration Measures For Neural Networks From Kernel Mean Embeddings,https://proceedings.mlr.press/v80/kumar18a/kumar18a.pdf,A Unifying Theory of Distance from Calibration,https://arxiv.org/pdf/2211.16886,2023,test
207,"[CITATION] propose two calibration measures to resolve this issue, which they call Adaptive Calibration Error (ACE) and Static Calibration Error (SCE). ACE and SCE extend ECE by measuring calibration over all classes in each bin, rather than just the predicted class.",Measuring Calibration in Deep Learning  ,https://openreview.net/forum?id=r1la7krKPS,The Calibration Generalization Gap,https://arxiv.org/pdf/2210.01964,2022,test
209,Data scaling of after-kernel (Section 4):We consider the after-kernel [CITATION] i.e. Empirical NTK extracted after training to completion on a fixed number of samples.,Properties of the After Kernel,https://arxiv.org/abs/2105.10585,Limitations of the NTK for Understanding Generalization in Deep Learning,https://arxiv.org/pdf/2206.10012,2022,test
211,"Initially proposed by [CITATION], model stitching is natural way of “plugging in” the bottom layers of one network into the top layers of another network, thus forming a stitched network (however care must be taken in the way it is performed, see Section 2)",Understanding image representations by measuring their equivariance and equivalence,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Lenc_Understanding_Image_Representations_2015_CVPR_paper.pdf,Revisiting Model Stitching to Compare Neural Representations,https://arxiv.org/pdf/2106.07682,2021,test
212,"On the other hand, on the IMDB sentiment corpus [CITATION] our attack increases classification test error from 12% to 23% with only 3% poisoned data, showing that defensibility is very dataset-dependent: the high dimensionality and abundance of irrelevant feature",Learning Word Vectors for Sentiment Analysis,https://aclanthology.org/P11-1015/,Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets,https://proceedings.neurips.cc/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf,2017,test
213,Generative Models. We further delve into the generation tasks for cartoon animation on SVD. We initialize the weights from SVD-MV [CITATION] and finetune the base model from stage II,Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets,https://arxiv.org/abs/2311.15127,Sakuga-42M Dataset: Scaling Up Cartoon Research,https://arxiv.org/pdf/2405.07425,2024,test
215,"Contrastive vision-language representation learning has recently emerged as an effective technique to learn representations with weak supervision that work for a wide range of tasks and have intriguing properties, such as strong zero-shot abilities. However, our understanding of the learned representation is in its infancy. For instance, recent work showed the presence of a modality gap and attributed it to the cone effect of model initialization and the contrastive loss [CITATION].",Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,https://arxiv.org/pdf/2203.02053.pdf,"Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning",https://arxiv.org/pdf/2404.07983,2024,test
216,"The influence of the modality gap on downstream performance is controversially discussed in the literature [36,53,73,74]. In the experiments from Fig. 4, ablating dimensions closed the modality gap but did not improve the downstream performance. Similarly, [CITATION] closed the gap by shifting the embeddings but found that an increase of the modality gap actually improved performance.",Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning,https://arxiv.org/pdf/2203.02053.pdf,"Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Representation Learning",https://arxiv.org/pdf/2404.07983,2024,test
218,"[CITATION] consists of an almost equal number of images from six different domains: Clipart, Infograph, Painting, Quickdraw, Real, and Sketch",Moment matching for multi-source domain adaptation,https://arxiv.org/pdf/1812.01754,Measuring Style Similarity in Diffusion Models,https://arxiv.org/pdf/2404.01292,2024,test
221,"Contrary, [CITATION] demonstrated that image classification models reach high accuracies on high-pass filtered images that mostly resemble noise and should not carry discriminative information.",High-frequency component helps explain the generalization of convolutional neural networks,https://arxiv.org/pdf/1905.13545,Can Biases in ImageNet Models Explain Generalization?,https://arxiv.org/pdf/2404.01509,2024,test
223,"The benchmark is based on the validation set of the ILSVRC-2012 challenge [CITATION]. We divide its 1,000 classes into 20 equally-sized subsets at random, with each subset functioning as a distinct task.",ImageNet: A Large-Scale Hierarchical Image Database,https://image-net.org/static_files/papers/imagenet_cvpr09.pdf,Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks,https://arxiv.org/pdf/2405.01719,2024,test
224,"To this end, we consider a notion of successful learning called list decodable learning, first introduced by [CITATION]. In analogy with list decodable coding theory, the goal is for the learning algorithm to output a short list of possible hypotheses.",Discriminative Framework for Clustering via Similarity Functions,https://www.cs.cmu.edu/~avrim/Papers/simclustering.pdf,Learning from Untrusted Data,https://arxiv.org/pdf/1611.02315,2017,test
227,"While testing on long sequences of images, both (Wang et al., 2022) and (Niu et al., 2022) show that Tent degrades in accuracy, the more iterations it does. [CITATION] show that this is in fact true for all TTA methods apart from EATA (Niu et al., 2022), which uses an L2 regularizer to constrain the adapting model’s weights to be close to those ofthe pretrained model.",RDumb: A simple approach that questions our progress in continual test-time adaptation,https://arxiv.org/pdf/2306.05401/,The Entropy Enigma: Success and Failure of Entropy Minimization,https://arxiv.org/pdf/2405.05012,2024,test
228,"As ImageNet contains many similar fine-grained classes, we restrict our analyses to the 16 classes outlined in [CITATION], which represent approximately 20% of the total images.",Generalisation in humans and deep neural networks,https://arxiv.org/pdf/1808.08750,The Entropy Enigma: Success and Failure of Entropy Minimization,https://arxiv.org/pdf/2405.05012,2024,test
229,"IPLoM [CITATION] used a series of heuristics to capture the differences of similar log messages to determine message types. Although these methods can successfully detect recurring patterns, they do so by considering textual properties of logs.",Clustering Event Logs Using Iterative Partitioning[TITLE_SEPARATOR]A Lightweight Algorithm for Message Type Extraction in System Application Logs,https://web.cs.dal.ca/~zincir/bildiri/kdd09-ane.pdf,Detecting Large-Scale System Problems by Mining,https://cs.uwaterloo.ca/~m2nagapp/courses/CS846/1171/papers/xu_sosp09.pdf,2009,test
231,RP trees are an alternative to k-means in which a distortion-reducing transformation is obtained via random projections [CITATION].,Random projection trees and low dimensional manifolds,https://cseweb.ucsd.edu/~dasgupta/papers/rptree-stoc.pdf,Fast Approximate Spectral Clustering,https://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2009-45.pdf,2009,test
232,"The affinity matrix of spectral clustering is a natural target for rank reduction. In particular, [CITATION] have used the Nystr¨om approximation, which samples columns of the affinity matrix and approximates the full matrix by using correlations between the sampled columns and the remaining columns.",Spectral Grouping Using the Nystr ¨om Method,https://math.ucdavis.edu/~saito/courses/ACHA.READ.S03/fbcm-pami-nystrom.pdf,Fast Approximate Spectral Clustering,https://digitalassets.lib.berkeley.edu/techreports/ucb/text/EECS-2009-45.pdf,2009,test
234,This benchmark [CITATION] contains 164 problems that involve generating of a Python program given a docstring and a function signature. A generation is considered correct if it passes all supplied unit tests. ,Evaluating large language models trained on code,https://arxiv.org/pdf/2107.03374,LoRA Learns Less and Forgets Less,https://arxiv.org/pdf/2405.09673,2024,test
237,"RCA [CITATION] is intermediate between PCA and LDA in its use of labeled data. Specifically, RCA makes use of so-called “chunklet” information, or subclass membership assignments.",Adjustment learning and relevant component analysis,https://www.semanticscholar.org/paper/Adjustment-Learning-and-Relevant-Component-Analysis-Shental-Hertz/5f619c286efec9931ac0f52d62bc149c62cdbf6e,Distance Metric Learning for Large Margin Nearest Neighbor Classification,https://jmlr.csail.mit.edu/papers/volume10/weinberger09a/weinberger09a.pdf,2009,test
238,"We use the ICLR 2018–2022 database assembled by [CITATION], which includes 10,297 papers.",Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach,https://arxiv.org/abs/2211.06398,Estimating the Causal Effect of Early ArXiving on Paper Acceptance,https://proceedings.mlr.press/v236/elazar24a/elazar24a.pdf,2024,test
239,"Absolute positional embeddings (APE) are learned embeddings that are added to token embeddings before the first layer of the transformer [Vaswani et al., 2017]. However, these absolute embeddings inhibit length generalization, first shown by [CITATION]","Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",https://arxiv.org/pdf/2108.12409,Transformers Can Do Arithmetic with the Right Embeddings,https://arxiv.org/pdf/2405.17399,2024,test
240,"Meanwhile, [CITATION] explores scaling laws in the context of board games, finding that “the compute required for a desired level of performance can be calculated directly from the board size”",Scaling Scaling Laws with Board Games,https://arxiv.org/pdf/2104.03113,gzip Predicts Data-dependent Scaling Laws,https://arxiv.org/pdf/2405.16684,2024,test
241,"Specifically, for each class in ImageNet, we parsed the Wikipedia content of the class following [CITATION], and then provided this content along with a prompt to GPT4 to generate five questions per class",Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions,https://arxiv.org/pdf/2103.09669,Why are Visually-Grounded Language Models Bad at Image Classification?,https://arxiv.org/pdf/2405.18415,2024,test
242,"Notably, [CITATION] find that ImageNet and OpenImages exhibit substantial US-centric and eurocentric representation bias",No classification without representation: Assessing geodiversity issues in open data sets for the developing world.,https://arxiv.org/pdf/1711.08536,Multilingual Diversity Improves Vision-Language Representations,https://arxiv.org/pdf/2405.16915,2024,test
243,"In particular, on [CITATION], which consists of images of common objects crowd-sourced from six different regions across the world, we find that using translated multilingual captions makes CLIP perform better on all regions, with the biggest gain coming from Africa images (5.5%) and the second biggest gain coming from the Europe region.",Geode: a geographically diverse evaluation dataset for object recognition,https://arxiv.org/pdf/2301.02560,Multilingual Diversity Improves Vision-Language Representations,https://arxiv.org/pdf/2405.16915,2024,test
244,"Another notable method is [CITATION], which utilizes full attention but gradually reduces the number of tokens in each transformer block by selecting the most representative tokens through bipartite matching for the Vision Transformer (ViT)",Token Merging: Your ViT But Faster,https://arxiv.org/pdf/2210.09461,Matryoshka Multimodal Models,https://arxiv.org/pdf/2405.17430,2024,test
246,"Since then, CLIP-like models have served as effective initializations and have been incorporated into various vision-language cross-modal models (e.g., video-text alignment [ 34 ,35 , 36, 37 ], large vision-language models [14, 15 , 38 ], etc.). Recently, [CITATION] introduced pairwise sigmoid loss during training, enabling the visual encoder to demonstrate more advanced visual perception capabilities",Sigmoid Loss for Language Image Pre-Training,https://arxiv.org/pdf/2303.15343,Dense Connector for MLLMs,https://arxiv.org/pdf/2405.13800,2024,test
247,"For instance, [CITATION] integrates a vision encoder from CLIP (Radford et al., 2021) with the LLM Vicuna (Chiang et al., 2023b), which is further fine-tuned on carefully constructed vision-language instructional datasets to activate the model’s perception capability of capturing the vision information according to different queries. ",Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485,Enhancing Large Vision Language Models with Self-Training on Image Comprehension,https://arxiv.org/pdf/2405.19716,2024,test
248,"We consider the widely used benchmarks for LVLM evaluation across different domains including: ScienceQA (Lu et al., 2022), TextVQA (Singh et al., 2019), ChartQA (Masry et al., 2022), LLaVA-Bench [CITATION], MMBench (Liu et al., 2023c), MM-Vet (Yu et al., 2023), and MathVista (Lu et al., 2024).",Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485,Enhancing Large Vision Language Models with Self-Training on Image Comprehension,https://arxiv.org/pdf/2405.19716,2024,test
249,"Recent work [CITATION] proposes a new, web-scale dataset for open-domain entity recognition. This challenging benchmark contains 6M entity names derived from Wikipedia page titles, including coarse-grained and fine-grained entities, encompassing a wide spectrum of concepts such as animals, buildings, organizations, landmarks, and a multitude of other. ",Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities,https://arxiv.org/pdf/2302.11154,A Generative Approach for Wikipedia-Scale Visual Entity Recognition,https://arxiv.org/pdf/2403.02041,2024,test
251,"ARO [CITATION] formulates the evaluation task as image-to-text retrieval problem, focusing on models’ understanding of different relationships, attributes, and order information.","When and why vision-language models behave like bags-of-words, and what to do about it?",https://arxiv.org/abs/2210.01936,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,https://arxiv.org/abs/2306.14610,2023,test
252,Seminal work DSI [CITATION] shows the benefit of learning to decode compact codes (created either randomly or with hierarchical k-means clustering) associated with each document.,Transformer Memory as a Differentiable Search Index,https://arxiv.org/abs/2202.06991,A Generative Approach for Wikipedia-Scale Visual Entity Recognition,https://arxiv.org/pdf/2403.02041,2024,test
254,"Agents receive instructions, documentation, and a demonstrations on the correct use of bash and ACI commands. At each step, agents are instructed to generate both a thought and an action [CITATION]",React: Synergizing reasoning and acting in language model,https://arxiv.org/pdf/2210.03629,SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering,https://arxiv.org/pdf/2405.15793,2024,test
256,"[CITATION] empirically determine that the effective context length of CLIP is less than 20 tokens and propose an algorithm to stretch the positional encoding, improving performance on longer texts",Long-CLIP: Unlocking the Long-Text Capability of CLIP,https://arxiv.org/pdf/2403.15378,Jina CLIP: Your CLIP Model Is Also Your Text Retriever,https://arxiv.org/pdf/2405.20204,2024,test
258,"We build upon GIT [CITATION], an auto-regressive image-to-text generative model. ",GIT: A Generative Image-to-text Transformer for Vision and Language,https://arxiv.org/abs/2205.14100,A Generative Approach for Wikipedia-Scale Visual Entity Recognition,https://arxiv.org/pdf/2403.02041,2024,test
260,TIGER [CITATION] studies generative retrieval in the context of recommender systems,Recommender Systems with Generative Retrieval,https://arxiv.org/abs/2305.05065,A Generative Approach for Wikipedia-Scale Visual Entity Recognition,https://arxiv.org/pdf/2403.02041,2024,test
261,"We evaluate DTV on GSM8K (Cobbe et al., 2021), three subsets of MATH (Hendrycks et al.,2021) following prior work (Zheng et al., 2021), and MultiArith [CITATION] datasets.",Solving General Arithmetic Word Problems,https://aclanthology.org/D15-1202/,Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization,https://arxiv.org/pdf/2403.18120,2024,test
263,For all of our experiments we set the initial learning rate to 1 and utilize the strong Wolfe line search method [CITATION] to compute the step sizes during optimization,Convergence Conditions for Ascent Methods,https://www.semanticscholar.org/paper/CONVERGENCE-CONDITIONS-FOR-ASCENT-METHODS-*-Philip-Wolfe/4c040ffd0bb4c3ef660114039b017095ac5ffd2d,IncDSI: Incrementally Updatable Document Retrieval,https://proceedings.mlr.press/v202/kishore23a/kishore23a.pdf,2023,test
267,"In the LaMem dataset [CITATION], each image is assigned a memorability score by humans in the task of identifying repeated images in a sequence",Understanding and Predicting Image Memorability at a Large Scale,https://people.csail.mit.edu/khosla/papers/iccv2015_khosla.pdf,Describing Differences in Image Sets with Natural Language,https://arxiv.org/pdf/2312.02974v1,2023,test
269,Einstein’s puzzle is a well-known logic puzzle often used as a benchmark for solving constraint satisfaction problems [CITATION],Hybrid Algorithms for the constraint satisfaction problem,https://www.dcs.gla.ac.uk/~pat/cpM/papers/CI9(3).pdf,Faith and Fate: Limits of Transformers on Compositionality,https://arxiv.org/pdf/2305.18654,2023,test
270,"Due to removing artifacts, Mamba®’s  feature maps [CITATION] appear more refined and concentrate on semantically important regions",Mamba-R: Vision Mamba ALSO Needs Registers,https://arxiv.org/pdf/2405.14858,Mamba-R: Vision Mamba ALSO Needs Registers,https://arxiv.org/pdf/2405.14858,2024,test
271,"AutoAttack consists of an ensemble of white- and black-box attacks, including APGD for cross-entropy and targeted DLR loss, FAB-attack [CITATION] and the black-box Square Attack [2]",Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack,https://arxiv.org/abs/1907.02044,Revisiting Adversarial Training at Scale,https://arxiv.org/pdf/2401.04727,2024,test
272,"Unlike [CITATION] which only appends several register tokens at one end of the input layer, we 1) insert register tokens evenly throughout the token sequence; and 2) at the end of the Vision Mamba, concatenate the register tokens to form a comprehensive image representation for the final prediction. ",Vision Transformers Need Registers,https://arxiv.org/abs/2309.16588,Mamba-R: Vision Mamba ALSO Needs Registers,https://arxiv.org/pdf/2405.14858,2024,test
273,"[CITATION] famously introduced a selective SSM block, that incorporates structured SSMs with hardware-aware state expansion, leading to a highly efficient recurrent architecture that is competitive to Transformer.",Mamba: Linear-Time Sequence Modeling with Selective State Spaces,https://arxiv.org/abs/2312.00752,Mamba-R: Vision Mamba ALSO Needs Registers,https://arxiv.org/pdf/2405.14858,2024,test
274,"[CITATION] investigates transformers’ theoretical expressiveness, showing that transformers cannot robustly model noncounter-free regular languages even when allowing infinite precision. ",Theoretical Limitations of Self-Attention in Neural Sequence Models,https://arxiv.org/abs/1906.06755,Faith and Fate: Limits of Transformers on Compositionality,https://arxiv.org/pdf/2305.18654,2023,test
275,"By discretizing this ordinary differential equation group, the continuous-time SSMs can be integrated to process discrete inputs such as language, speech, and image pixels. To this end, the model can be solved by an analytic solution and then approximated by Zero-Order Hold [CITATION], leading to a discrete model.",Mamba: Linear-Time Sequence Modeling with Selective State Spaces,https://arxiv.org/abs/2312.00752,Mamba-R: Vision Mamba ALSO Needs Registers,https://arxiv.org/pdf/2405.14858,2024,test
276,"Current research have also provided valuable insights into the grokking phenomenon from the dynamics of learning. [CITATION] reported that the grokking happens exclusively in accordance to a slingshot effect, which corresponding to a cyclic phase transitions between stable-and-unstable training regimes and the norm growth-and-plateau of the last layer weights",The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon,https://arxiv.org/pdf/2206.04817,Deep Grokking: Would Deep Neural Networks Generalize Better?,https://arxiv.org/pdf/2405.19454,2024,test
277,"Recently, the grokking phenomenon was proposed by [CITATION], in the context of studying the optimization and generalization aspects in small, algorithmically generated datasets. Specifically, grokking refers to a sudden transition from chance level validation accuracy to perfect generalization, long past the point of perfect training accuracy, i.e., Terminal Phase of Training (TPT).",Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,https://arxiv.org/pdf/2201.02177,The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon,https://arxiv.org/pdf/2206.04817,2024,test
